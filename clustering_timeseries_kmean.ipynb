{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Contents**\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "    - [1.1. What is clustering ?](#1.-1.-What-is-clustering-?)\n",
    "    - [1.2. What is time series ?](#1.-2.-What-are-time-series-?)\n",
    "- [2. Analysis](#2.-Analysis)\n",
    "    - [2.1. Let's check the data](#2.-1.-Let's-check-the-data)\n",
    "    - [2.2. Preprocessing](#2.-2.-Preprocessing)\n",
    "    - [2.3. Clustering](#2.-3.-Clustering)\n",
    "        - [2.3.1. SOM](#2.-3.-1.-SOM)\n",
    "           - [2.3.1.1. Results](#2.-3.-1.-1.-Results)\n",
    "           - [2.3.1.2. Cluster Distribution](#2.-3.-1.-2.-Cluster-Distribution)\n",
    "           - [2.3.1.3. Cluster Mapping](#2.-3.-1.-3.-Cluster-Mapping)\n",
    "        - [2.3.2. K-Means](#2.-3.-2.-K-Means)\n",
    "            - [2.3.2.1. Results](#2.-3.-2.-1.-Results)\n",
    "            - [2.3.2.2. Cluster Distribution](#2.-3.-2.-2.-Cluster-Distribution)\n",
    "            - [2.3.2.3. Cluster Mapping](#2.-3.-2.-3.-Cluster-Mapping)\n",
    "            - [2.3.2.4. Curse of Dimensionality](#2.-3.-2.-4.-Curse-of-Dimensionality)\n",
    "- [3. Libraries](#3.-Libraries)\n",
    "- [4. References](#4.-References)\n",
    "- [5. See Also](#5.-See-Also)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "## 1. 1. What is clustering ?\n",
    "\n",
    "   Clustering is a type of unsupervised learning problem and the main idea is finding similarities between different data points and pair them under the same group in a way that those data points in the same group (cluster) are more like each other than to those in other groups. It is one of the main tasks of exploratory data mining and used in many fields such as bioinformatics, pattern recognition, image analysis, machine learning, etc.\n",
    "    \n",
    "![Clustering algorithms benchmark](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png) \n",
    "<center>Source : scikit-learn Documentation: <a href=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png\">sphx_glr_plot_cluster_comparison_0011.png</a></center>\n",
    "    \n",
    "## 1. 2. What are time series ?\n",
    "    \n",
    "   Time series are a stream of data that are created by making measures of something such as sales, temperature, stocks, etc. in fixed frequency. They have to be indexed in time order and usually used in weather forecasting, econometrics, earthquake prediction, signal processing, etc.\n",
    "    \n",
    "![Time series example](https://upload.wikimedia.org/wikipedia/commons/7/77/Random-data-plus-trend-r2.png)\n",
    "<center>Source : Wiki Commons: <a href=\"https://upload.wikimedia.org/wikipedia/commons/7/77/Random-data-plus-trend-r2.png\">Random-data-plus-trend-r2.png</a></center>\n",
    "\n",
    "# 2. Analysis\n",
    "\n",
    "In this notebook, we will be using [Retail and Retailers Sales Time Series Collection](https://www.kaggle.com/census/retail-and-retailers-sales-time-series-collection) that is provided by [US Census Bureau](https://www.kaggle.com/census).\n",
    "\n",
    "## 2. 1. Let's check the data\n",
    "\n",
    "First of all, let's read it from the input and put them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minisom\n",
      "  Downloading MiniSom-2.3.0.tar.gz (8.8 kB)\n",
      "Building wheels for collected packages: minisom\n",
      "  Building wheel for minisom (setup.py): started\n",
      "  Building wheel for minisom (setup.py): finished with status 'done'\n",
      "  Created wheel for minisom: filename=MiniSom-2.3.0-py3-none-any.whl size=9021 sha256=f4e2001a14e395eae5bdbb699295ed0cff758e323810380f183982ba2d4fcbc2\n",
      "  Stored in directory: c:\\users\\nicolas\\appdata\\local\\pip\\cache\\wheels\\7e\\47\\6d\\97ad48be13d8b0fc231b7df226a3d6645820c32559822a826c\n",
      "Successfully built minisom\n",
      "Installing collected packages: minisom\n",
      "Successfully installed minisom-2.3.0\n",
      "Requirement already satisfied: tslearn in c:\\miniconda\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: numba in c:\\miniconda\\lib\\site-packages (from tslearn) (0.56.4)\n",
      "Requirement already satisfied: joblib in c:\\miniconda\\lib\\site-packages (from tslearn) (1.2.0)\n",
      "Requirement already satisfied: Cython in c:\\miniconda\\lib\\site-packages (from tslearn) (0.29.32)\n",
      "Requirement already satisfied: scipy in c:\\miniconda\\lib\\site-packages (from tslearn) (1.9.2)\n",
      "Requirement already satisfied: numpy in c:\\miniconda\\lib\\site-packages (from tslearn) (1.23.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\miniconda\\lib\\site-packages (from tslearn) (1.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\miniconda\\lib\\site-packages (from numba->tslearn) (61.2.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\miniconda\\lib\\site-packages (from numba->tslearn) (0.39.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\miniconda\\lib\\site-packages (from scikit-learn->tslearn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install minisom\n",
    "!pip install tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Native libraries\n",
    "import os\n",
    "import math\n",
    "# Essential Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Algorithms\n",
    "from minisom import MiniSom\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: '/kaggle/input/retail-and-retailers-sales-time-series-collection/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m mySeries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m namesofMySeries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      7\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(directory\u001b[38;5;241m+\u001b[39mfilename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Le chemin d’accès spécifié est introuvable: '/kaggle/input/retail-and-retailers-sales-time-series-collection/'"
     ]
    }
   ],
   "source": [
    "directory = '/kaggle/input/retail-and-retailers-sales-time-series-collection/'\n",
    "\n",
    "mySeries = []\n",
    "namesofMySeries = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        df = pd.read_csv(directory+filename)\n",
    "        df = df.loc[:,[\"date\",\"value\"]]\n",
    "        # While we are at it I just filtered the columns that we will be working on\n",
    "        df.set_index(\"date\",inplace=True)\n",
    "        # ,set the date columns as index\n",
    "        df.sort_index(inplace=True)\n",
    "        # and lastly, ordered the data according to our date index\n",
    "        mySeries.append(df)\n",
    "        namesofMySeries.append(filename[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many series we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mySeries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for 23 series let's create a 6 by 4 grid which will be resulted in 24 slots and fill it with the plot of our series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6,4,figsize=(25,25))\n",
    "fig.suptitle('Series')\n",
    "for i in range(6):\n",
    "    for j in range(4):\n",
    "        if i*4+j+1>len(mySeries): # pass the others that we can't fill\n",
    "            continue\n",
    "        axs[i, j].plot(mySeries[i*4+j].values)\n",
    "        axs[i, j].set_title(namesofMySeries[i*4+j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there are pretty much similar time series such as ```MRTSSM44000USS``` and ```RETAILMSA``` or ```MRTSSM7221USN``` and ```MRTSSM44611USN```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6,4,figsize=(25,25))\n",
    "fig.suptitle('Series')\n",
    "for i in range(6):\n",
    "    for j in range(4):\n",
    "        if i*4+j+1>len(mySeries): # pass the others that we can't fill\n",
    "            continue\n",
    "        axs[i, j].plot(mySeries[i*4+j].values)\n",
    "        axs[i, j].set_title(namesofMySeries[i*4+j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 2. Preprocessing\n",
    "\n",
    "Before we start analyzing let's check if our data is uniform in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_lengths = {len(series) for series in mySeries}\n",
    "print(series_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we guessed, it is not uniform in length. So in this case, we should find which series contain missing data and fill them. Because, otherwise our indices will be shifted and i.th index -let's say it is 10th of May- of the x series won't be same as i.th index of the y series -let's say i.th index of the y series may be 11th of May-."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "for series in mySeries:\n",
    "    print(\"[\"+str(ind)+\"] \"+series.index[0]+\" \"+series.index[len(series)-1])\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see 6th, 11th and 12th series are not starting from the same date as others. To solve this problem, we should first find the longest series of the series and elongate others according to that. Usually, to do this we should check the oldest and newest date and elongate all series according to these dates. But in our case, nearly every series starts from 1992-01-01 and ends in 2019-09-01. Thus, finding the longest series will be enough for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(series_lengths)\n",
    "longest_series = None\n",
    "for series in mySeries:\n",
    "    if len(series) == max_len:\n",
    "        longest_series = series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code block, I reindexed the series that are not as long as the longest one and fill the empty dates with ```np.nan```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_index = []\n",
    "\n",
    "for i in range(len(mySeries)):\n",
    "    if len(mySeries[i])!= max_len:\n",
    "        problems_index.append(i)\n",
    "        mySeries[i] = mySeries[i].reindex(longest_series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many series are polluted with nan values with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_counter(list_of_series):\n",
    "    nan_polluted_series_counter = 0\n",
    "    for series in list_of_series:\n",
    "        if series.isnull().sum().sum() > 0:\n",
    "            nan_polluted_series_counter+=1\n",
    "    print(nan_polluted_series_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 series that are polluted with nan and we used to have 3 series that are shorter than others, so math checks out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counter(mySeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these series lack only one point, I used linear interpolation to fill the gap but for series that have more missing value, you can use much more complex interpolation methods such as quadratic, cubic, spline, barycentric, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in problems_index:\n",
    "    mySeries[i].interpolate(limit_direction=\"both\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, now all of our series are the same length and don't contain any missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counter(mySeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After handling missing values, the other issue is the scale of the series. Without, normalizing data the series that looks like each other will be seen so different from each other and will affect the accuracy of the clustering process. We can see the effect of the normalizing in the following images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "a = [[2],[7],[11],[14],[19],[23],[26]]\n",
    "b = [[20000000],[40000000],[60000000],[80000000],[100000000],[120000000],[140000000]]\n",
    "fig, axs = plt.subplots(1,3,figsize=(25,5))\n",
    "axs[0].plot(a)\n",
    "axs[0].set_title(\"Series 1\")\n",
    "axs[1].plot(b)\n",
    "axs[1].set_title(\"Series 2\")\n",
    "axs[2].plot(a)\n",
    "axs[2].plot(b)\n",
    "axs[2].set_title(\"Series 1 & 2\")\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(MinMaxScaler().fit_transform(a))\n",
    "plt.plot(MinMaxScaler().fit_transform(b))\n",
    "plt.title(\"Normalized Series 1 & Series 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we normalized each time series by their own values, not the values of other time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mySeries)):\n",
    "    scaler = MinMaxScaler()\n",
    "    mySeries[i] = MinMaxScaler().fit_transform(mySeries[i])\n",
    "    mySeries[i]= mySeries[i].reshape(len(mySeries[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the normalizing process seems fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max: \"+str(max(mySeries[0]))+\"\\tmin: \"+str(min(mySeries[0])))\n",
    "print(mySeries[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 3. Clustering\n",
    "\n",
    "I will be using 2 different methods for clustering these series. The first of the methods is Self Organizing Maps(SOM) and the other method is K-Means.\n",
    "\n",
    "### 2. 3. 1. SOM\n",
    " \n",
    "Self-organizing maps are a type of neural network that is trained using unsupervised learning to produce a low-dimensional representation of the input space of the training samples, called a map.\n",
    "\n",
    "![SOM](https://raw.githubusercontent.com/izzettunc/Kohonen-SOM/master/data/screenshots/landing.png)\n",
    "<center>Source : Github Repo: <a href=\"https://raw.githubusercontent.com/izzettunc/Kohonen-SOM/master/data/screenshots/landing.png\">landing.png</a></center>\n",
    "<br>    \n",
    "Also, self-organizing maps  differ from other artificial neural networks as they apply competitive(or cooperative) learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.\n",
    "\n",
    "![Learning process of som](https://upload.wikimedia.org/wikipedia/commons/3/35/TrainSOM.gif)\n",
    "<center>Source : Wiki Commons: <a href=\"https://upload.wikimedia.org/wikipedia/commons/3/35/TrainSOM.gif\">TrainSOM.gif</a></center>\n",
    "<br>\n",
    "Because of the ability to produce a map, som deemed as a method to do dimensionality reduction. But in our case, when each node of the som is accepted as medoids of the cluster, we can use it for clustering. To do so, we should remove our time indices from our time series, and instead of measured values of each date, we should accept them as different features and dimensions of a single data point.\n",
    "\n",
    "For more info about some, you can check [this medium post](https://medium.com/@abhinavr8/self-organizing-maps-ff5853a118d4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "a = [1,2]\n",
    "b = [3,7]\n",
    "c = [1,3]\n",
    "d = [3,8]\n",
    "img = plt.imread(\"/kaggle/input/notebook-material/arrow.png\")\n",
    "fig, axs = plt.subplots(1,3,figsize=(25,5))\n",
    "axs[0].plot(a)\n",
    "axs[0].plot(b)\n",
    "axs[0].plot(c)\n",
    "axs[0].plot(d)\n",
    "axs[0].set_title(\"Time Series\")\n",
    "axs[1].imshow(img)\n",
    "axs[1].axis(\"off\")\n",
    "axs[2].set_title(\"Data Points\")\n",
    "axs[2].scatter(a[0],a[1], s=300)\n",
    "axs[2].scatter(b[0],b[1], s=300)\n",
    "axs[2].scatter(c[0],c[1], s=300)\n",
    "axs[2].scatter(d[0],d[1], s=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the implementation of the som algorithm I used [miniSom](https://github.com/JustGlowing/minisom) and set my parameters as follows:\n",
    "- sigma: 0.3\n",
    "- learning_rate: 0.5\n",
    "- random weight initialization\n",
    "- 50.000 iteration\n",
    "- Map size: square root of the number of series\n",
    "\n",
    "As a side note, I didn't optimize these parameters due to the simplicity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(mySeries))))\n",
    "# I didn't see its significance but to make the map square,\n",
    "# I calculated square root of map size which is \n",
    "# the square root of the number of series\n",
    "# for the row and column counts of som\n",
    "\n",
    "som = MiniSom(som_x, som_y,len(mySeries[0]), sigma=0.3, learning_rate = 0.1)\n",
    "\n",
    "som.random_weights_init(mySeries)\n",
    "som.train(mySeries, 50000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 3. 1. 1. Results\n",
    "\n",
    "After the training, I plotted the results. For each cluster, I plotted every series, a little bit transparent and in gray, and in order to see the movement or the shape of the cluster, I took the average of the cluster and plotted that averaged series in red ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little handy function to plot series\n",
    "def plot_som_series_averaged_center(som_x, som_y, win_map):\n",
    "    fig, axs = plt.subplots(som_x,som_y,figsize=(25,25))\n",
    "    fig.suptitle('Clusters')\n",
    "    for x in range(som_x):\n",
    "        for y in range(som_y):\n",
    "            cluster = (x,y)\n",
    "            if cluster in win_map.keys():\n",
    "                for series in win_map[cluster]:\n",
    "                    axs[cluster].plot(series,c=\"gray\",alpha=0.5) \n",
    "                axs[cluster].plot(np.average(np.vstack(win_map[cluster]),axis=0),c=\"red\")\n",
    "            cluster_number = x*som_y+y+1\n",
    "            axs[cluster].set_title(f\"Cluster {cluster_number}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "win_map = som.win_map(mySeries)\n",
    "# Returns the mapping of the winner nodes and inputs\n",
    "\n",
    "plot_som_series_averaged_center(som_x, som_y, win_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the plot below, som perfectly clustered the 23 different series into 8 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "win_map = som.win_map(mySeries)\n",
    "# Returns the mapping of the winner nodes and inputs\n",
    "\n",
    "plot_som_series_averaged_center(som_x, som_y, win_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method to extract the movement/shape of the cluster is instead of averaging each series in the cluster, using Dynamic Time Warping Barycenter Averaging ([DBA](https://github.com/fpetitjean/DBA)).\n",
    "\n",
    "DBA is another type of averaging method that used the Dynamic Time Warping method in it and might be very useful to extract the movement/shape of the cluster as seen in the following images.\n",
    "\n",
    "![Arithmetic Averaging](https://raw.githubusercontent.com/fpetitjean/DBA/master/images/arithmetic.png)\n",
    "![DBA](https://raw.githubusercontent.com/fpetitjean/DBA/master/images/DBA.png)\n",
    "\n",
    "\n",
    "To do so, I used ```dtw_barycenter_averaging``` method in the [tslearn](https://github.com/tslearn-team/tslearn) library and changed the ```np.average``` with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_som_series_dba_center(som_x, som_y, win_map):\n",
    "    fig, axs = plt.subplots(som_x,som_y,figsize=(25,25))\n",
    "    fig.suptitle('Clusters')\n",
    "    for x in range(som_x):\n",
    "        for y in range(som_y):\n",
    "            cluster = (x,y)\n",
    "            if cluster in win_map.keys():\n",
    "                for series in win_map[cluster]:\n",
    "                    axs[cluster].plot(series,c=\"gray\",alpha=0.5) \n",
    "                axs[cluster].plot(dtw_barycenter_averaging(np.vstack(win_map[cluster])),c=\"red\") # I changed this part\n",
    "            cluster_number = x*som_y+y+1\n",
    "            axs[cluster].set_title(f\"Cluster {cluster_number}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "win_map = som.win_map(mySeries)\n",
    "\n",
    "plot_som_series_dba_center(som_x, som_y, win_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't see much difference from this result but, I highly recommend that to use this method for this purpose. But, also, note that the operation of dba is not a light one. So, if you seek speed, this method might not be for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "win_map = som.win_map(mySeries)\n",
    "\n",
    "plot_som_series_dba_center(som_x, som_y, win_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 3. 1. 2. Cluster Distribution\n",
    "We can see the distribution of the time series in clusters in the following chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "cluster_c = []\n",
    "cluster_n = []\n",
    "for x in range(som_x):\n",
    "    for y in range(som_y):\n",
    "        cluster = (x,y)\n",
    "        if cluster in win_map.keys():\n",
    "            cluster_c.append(len(win_map[cluster]))\n",
    "        else:\n",
    "            cluster_c.append(0)\n",
    "        cluster_number = x*som_y+y+1\n",
    "        cluster_n.append(f\"Cluster {cluster_number}\")\n",
    "\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.title(\"Cluster Distribution for SOM\")\n",
    "plt.bar(cluster_n,cluster_c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 3. 1. 3. Cluster Mapping\n",
    "\n",
    "<p style=\"color:gray\">(Thank you for this wonderful question <a href=\"https://www.kaggle.com/stephentseng\">Stephen Tseng</a>)</p>\n",
    "Well, we did cluster our series but how de we know which series belonging to which cluster? Ain't that the whole purpose of clustering? <br><br>\n",
    "\n",
    "As we can see in [these illustrations](#2.-3.-1.-SOM) each node (or multiple of nodes in some cases) represents a cluster. Therefore we can find out which series is belonging to which cluster by checking the winner node of each series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check first 5\n",
    "for series in mySeries[:5]:\n",
    "    print(som.winner(series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this piece of information more appealing to eye, we can map each node to a number <br>\n",
    "\n",
    "```e.g. for n*m grid (0,0)=1, (0,1)=2, ... (0,m)=m+1, (1,0)=(m+1)+1, (1,1)=(m+1)+2, ... , (n,m)=(n+1)*(m+1) ``` \n",
    "\n",
    "and print the name of the series with the cluster number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map = []\n",
    "for idx in range(len(mySeries)):\n",
    "    winner_node = som.winner(mySeries[idx])\n",
    "    cluster_map.append((namesofMySeries[idx],f\"Cluster {winner_node[0]*som_y+winner_node[1]+1}\"))\n",
    "\n",
    "pd.DataFrame(cluster_map,columns=[\"Series\",\"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 3. 2. K-Means\n",
    "\n",
    "K-means clustering is a method that aims to cluster n input to k clusters in which each data point belongs to cluster with the nearest mean (cluster centroid). It can be visualized as Voronoi cells and it is one of the most popular clustering algorithms and the most basic one. For more info about k-means, you can check [this medium post](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a).\n",
    "\n",
    "![Training process](https://i.imgur.com/k4XcapI.gif)\n",
    "\n",
    "In order to cluster our series with k-means, the essential thing to do is, as we do it with som, removing our time indices from our time series, and instead of measured values of each date, we should accept them as different features and dimensions of a single data point. Another important thing to do is, selecting the distance metric. In the k-means algorithm, people usually use the euclidean distance but as we've seen in [DBA](https://github.com/fpetitjean/DBA), it is not effective in our case. So, we will be using Dynamic Time Warping (DTW) instead of euclidean distance and you can see why we are doing this in the following images.\n",
    "\n",
    "![Difference of dtw and euclidean distance](https://upload.wikimedia.org/wikipedia/commons/6/69/Euclidean_vs_DTW.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = math.ceil(math.sqrt(len(mySeries))) \n",
    "# A good rule of thumb is choosing k as the square root of the number of points in the training data set in kNN\n",
    "\n",
    "km = TimeSeriesKMeans(n_clusters=cluster_count, metric=\"dtw\")\n",
    "\n",
    "labels = km.fit_predict(mySeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 3. 2. 1. Results\n",
    "\n",
    "After the training, I plotted the results as I did with the som. For each cluster, I plotted every series, a little bit transparent and in gray, and in order to see the movement or the shape of the cluster, I took the average of the cluster and plotted that averaged series in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_count = math.ceil(math.sqrt(cluster_count))\n",
    "\n",
    "fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\n",
    "fig.suptitle('Clusters')\n",
    "row_i=0\n",
    "column_j=0\n",
    "# For each label there is,\n",
    "# plots every series with that label\n",
    "for label in set(labels):\n",
    "    cluster = []\n",
    "    for i in range(len(labels)):\n",
    "            if(labels[i]==label):\n",
    "                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n",
    "                cluster.append(mySeries[i])\n",
    "    if len(cluster) > 0:\n",
    "        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n",
    "    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n",
    "    column_j+=1\n",
    "    if column_j%plot_count == 0:\n",
    "        row_i+=1\n",
    "        column_j=0\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the plot below, k-means clustered the 23 different series into 5 clusters. 2 of the clusters contains only 1 time series which may be deemed as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plot_count = math.ceil(math.sqrt(cluster_count))\n",
    "\n",
    "fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\n",
    "fig.suptitle('Clusters')\n",
    "row_i=0\n",
    "column_j=0\n",
    "# For each label there is,\n",
    "# plots every series with that label\n",
    "for label in set(labels):\n",
    "    cluster = []\n",
    "    for i in range(len(labels)):\n",
    "            if(labels[i]==label):\n",
    "                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n",
    "                cluster.append(mySeries[i])\n",
    "    if len(cluster) > 0:\n",
    "        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n",
    "    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n",
    "    column_j+=1\n",
    "    if column_j%plot_count == 0:\n",
    "        row_i+=1\n",
    "        column_j=0\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I did before, I used [DBA](https://github.com/fpetitjean/DBA) to see much more time dilated series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count = math.ceil(math.sqrt(cluster_count))\n",
    "\n",
    "fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\n",
    "fig.suptitle('Clusters')\n",
    "row_i=0\n",
    "column_j=0\n",
    "for label in set(labels):\n",
    "    cluster = []\n",
    "    for i in range(len(labels)):\n",
    "            if(labels[i]==label):\n",
    "                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n",
    "                cluster.append(mySeries[i])\n",
    "    if len(cluster) > 0:\n",
    "        axs[row_i, column_j].plot(dtw_barycenter_averaging(np.vstack(cluster)),c=\"red\")\n",
    "    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n",
    "    column_j+=1\n",
    "    if column_j%plot_count == 0:\n",
    "        row_i+=1\n",
    "        column_j=0\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 3. 2. 2. Cluster Distribution\n",
    "\n",
    "We can see the distribution of the time series in clusters in the following chart. And it seems like k-means clustered 15 of the time series as cluster 1, which is a bit skewed. The reason why this happens is the most probably ```The Curse of Dimentionality``` <p><small><small>You can check it out from the links that I provided at section 5 (See Also)</small></small></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "cluster_c = [len(labels[labels==i]) for i in range(cluster_count)]\n",
    "cluster_n = [\"Cluster \"+str(i) for i in range(cluster_count)]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Cluster Distribution for KMeans\")\n",
    "plt.bar(cluster_n,cluster_c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 3. 2. 3. Cluster Mapping\n",
    "\n",
    "As we did before, in this part we will be finding which series belonging to which cluster. Thanks to awesome scikit-learn library we actually already have that information. Order of the labels is the same order with our series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_names_for_labels = [f\"Cluster {label}\" for label in labels]\n",
    "pd.DataFrame(zip(namesofMySeries,fancy_names_for_labels),columns=[\"Series\",\"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 3. 2. 4. Curse of Dimensionality\n",
    "\n",
    "Curse of Dimensionality is a term, first invented by Richard E. Bellman when considering problems in dynamic programming. It basically means, when the dimensionality of the data increase so does the distance between data points. Thus, this change in measurement of distance affects the distance-based algorithms badly. To learn for more about it please check section [5. See Also](#5.-See-Also).\n",
    "\n",
    "To solve this problem there are numerous algorithms that can be helpful such as PCA which is the most prominent of them, t-SNE, UMAP(map of the som), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "mySeries_transformed = pca.fit_transform(mySeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with less dimension than before, we can see how our series distributed in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "plt.scatter(mySeries_transformed[:,0],mySeries_transformed[:,1], s=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of PCA is basically, representation of a 333-dimensional data point as a 2-dimensional data point. As a result of that instead of a time series, we have just 2 value for each series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mySeries_transformed[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we don't have to use ```dtw``` anymore and instead of ```TimeSeriesKMeans``` from tslearn, we can use basic ```KMeans``` from ```sklearn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=cluster_count,max_iter=5000)\n",
    "\n",
    "labels = kmeans.fit_predict(mySeries_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the result of the basic KMeans, pretty logical and straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "plt.scatter(mySeries_transformed[:, 0], mySeries_transformed[:, 1], c=labels, s=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again thanks to the clever implementation of ```KMeans``` algorithm by ```sklearn``` team, labels are returned in the same order. Thus, we can use the same code to visualize our cluster in series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "plot_count = math.ceil(math.sqrt(cluster_count))\n",
    "\n",
    "fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\n",
    "fig.suptitle('Clusters')\n",
    "row_i=0\n",
    "column_j=0\n",
    "for label in set(labels):\n",
    "    cluster = []\n",
    "    for i in range(len(labels)):\n",
    "            if(labels[i]==label):\n",
    "                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n",
    "                cluster.append(mySeries[i])\n",
    "    if len(cluster) > 0:\n",
    "        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n",
    "    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n",
    "    column_j+=1\n",
    "    if column_j%plot_count == 0:\n",
    "        row_i+=1\n",
    "        column_j=0\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that now with the ```PCA``` algorithm, our series are much more equally distributed to clusters than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "cluster_c = [len(labels[labels==i]) for i in range(cluster_count)]\n",
    "cluster_n = [\"cluster_\"+str(i) for i in range(cluster_count)]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Cluster Distribution for KMeans\")\n",
    "plt.bar(cluster_n,cluster_c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_names_for_labels = [f\"Cluster {label}\" for label in labels]\n",
    "pd.DataFrame(zip(namesofMySeries,fancy_names_for_labels),columns=[\"Series\",\"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Libraries\n",
    "\n",
    "In here, you can easily reach the libraries that I used in this notebook.\n",
    "- [Pandas](https://github.com/pandas-dev/pandas)\n",
    "- [NumPy](https://github.com/numpy/numpy)\n",
    "- [scikit-learn](https://github.com/scikit-learn/scikit-learn)\n",
    "- [MiniSom](https://github.com/JustGlowing/minisom)\n",
    "- [tslearn](https://github.com/tslearn-team/tslearn)\n",
    "- [matplotlib](https://matplotlib.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. References\n",
    "\n",
    "- Petitjean F., Ketterlin A., Gançarski P., A global averaging method for dynamic time warping, with applications to clustering, Pattern Recognition, 44(3), 678-693, 2011\n",
    "- Kohonen T., Self-organized formation of topologically correct feature maps, Biological Cybernetics, 43, 59–69, 1982\n",
    "- Bellman R., Kalaba R., On adaptive control processes, in IRE Transactions on Automatic Control, 4(2), 1-9, 1959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. See Also\n",
    "\n",
    "* [K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)\n",
    "* [Self Organizing Maps](https://medium.com/@abhinavr8/self-organizing-maps-ff5853a118d4)\n",
    "* <p style=\"color:red\"><a href=\"https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e\">The Curse of Dimensionality</a> <b>***</b></p>\n",
    "* <p style=\"color:red\"><a href=\"https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d\">k-Nearest Neighbors and the Curse of Dimensionality</a> <b>***</b></p>\n",
    "* <p style=\"color:red\"><a href=\"https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0\">Understanding Principal Component Analysisy</a> <b>***</b></p>\n",
    "* <p style=\"color:red\"><a href=\"https://www.youtube.com/watch?v=FgakZw6K1QQ&ab_channel=StatQuestwithJoshStarmer\">StatQuest: Principal Component Analysis (PCA), Step-by-Step</a> <b>*** (An awesome video by StatQuest)</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, this is my first notebook and tutorial in Kaggle, so feel free to criticize and comment about, any error that you see, any idea of improvement for this notebook, or questions that you have.\n",
    "\n",
    "Have a nice day!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
